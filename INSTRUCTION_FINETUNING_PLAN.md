# ğŸ¯ Instruction Fine-Tuning Plan: GPT-2 â†’ Chat Assistant

## ğŸ“‹ Executive Summary

**Goal**: Transform your pre-trained GPT-2 model into a simple instruction-following chat assistant.

**Approach**: Fine-tune on instruction-response pairs using a smaller, high-quality dataset.

**Resources**: Your existing training infrastructure (Blackwell GPU, 120GB memory, training scripts).

**Timeline**: ~1-2 days of fine-tuning (vs. 34 days for pre-training).

---

## ğŸ§  What is Instruction Fine-Tuning?

### Pre-training (What you just did) âœ…
- **Dataset**: Raw text from the web (FineWeb-Edu)
- **Task**: Next-token prediction
- **Goal**: Learn language patterns, facts, reasoning
- **Output**: General language model (autocomplete style)

### Instruction Fine-Tuning (What we'll do now) ğŸ¯
- **Dataset**: Instruction-response pairs (Q&A, chat, tasks)
- **Task**: Follow instructions and generate helpful responses
- **Goal**: Align model behavior to user requests
- **Output**: Chat assistant that responds to prompts

**Key Insight**: You've already done the expensive part (pre-training). Instruction fine-tuning is much faster and cheaper!

---

## ğŸ“Š Dataset Selection

### Recommended Datasets (Ranked by Size & Quality)

#### ğŸ¥‡ **Option 1: Dolly-15k** (BEST for limited resources)
- **Name**: `databricks/databricks-dolly-15k`
- **Size**: 15,000 instruction-response pairs
- **Quality**: High (human-generated by Databricks employees)
- **Categories**: Open QA, closed QA, creative writing, information extraction, summarization
- **Training Time**: ~2-4 hours on your GPU
- **Why**: Small, clean, diverse, perfect for small models

```python
from datasets import load_dataset
dataset = load_dataset("databricks/databricks-dolly-15k")
```

#### ğŸ¥ˆ **Option 2: Alpaca GPT-4** (Good quality, larger)
- **Name**: `vicgalle/alpaca-gpt4`
- **Size**: 52,000 instruction-response pairs
- **Quality**: High (generated by GPT-4)
- **Training Time**: ~6-12 hours on your GPU
- **Why**: More diverse, but 3x larger than Dolly

```python
from datasets import load_dataset
dataset = load_dataset("vicgalle/alpaca-gpt4")
```

#### ğŸ¥‰ **Option 3: Anthropic HH (helpful/harmless)** (Conversation style)
- **Name**: `Anthropic/hh-rlhf`
- **Size**: ~160,000 conversations
- **Quality**: High (human-written conversations)
- **Training Time**: ~1-2 days
- **Why**: More conversational, but much larger

```python
from datasets import load_dataset
dataset = load_dataset("Anthropic/hh-rlhf")
```

#### ğŸ† **RECOMMENDED: Start with Dolly-15k**
- Fastest to train
- High quality
- Good results for small models
- Can always continue with Alpaca later

---

## ğŸ”§ Technical Approach

### Phase 1: Data Preparation

#### Chat Format Template

GPT-2 doesn't have special tokens for chat, so we'll use a simple format:

```
### Instruction:
{user instruction/question}

### Response:
{model response}
```

Example:
```
### Instruction:
What is machine learning?

### Response:
Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It uses algorithms to identify patterns and make decisions based on examples.
```

#### Data Processing Script

```python
def format_instruction(example):
    """
    Convert instruction dataset to GPT-2 training format.
    For Dolly-15k: {instruction, context, response, category}
    """
    instruction = example['instruction']
    context = example.get('context', '')
    response = example['response']
    
    # Build prompt
    if context:
        prompt = f"### Instruction:\n{instruction}\n\n### Context:\n{context}\n\n### Response:\n{response}"
    else:
        prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{response}"
    
    return {"text": prompt}
```

### Phase 2: Training Modifications

#### Key Differences from Pre-training

| Aspect | Pre-training | Instruction Fine-tuning |
|--------|-------------|------------------------|
| Dataset | Raw text | Instruction-response pairs |
| Format | Continuous text | Structured template |
| Learning Rate | 6e-4 | 1e-5 to 5e-5 (lower!) |
| Epochs | 1 epoch | 3-5 epochs |
| Time | Weeks | Hours to days |
| Loss Masking | No | Yes (only compute loss on response) |

#### Hyperparameter Recommendations

```python
# For Dolly-15k (15k samples)
args = {
    'learning_rate': 2e-5,          # Lower than pre-training
    'num_epochs': 3,                # Multiple passes
    'mini_batch_size': 8,           # Smaller batches
    'total_batch_size': 128,        # Effective batch size
    'max_steps': 500,               # ~3 epochs with 15k samples
    'warmup_steps': 50,             # 10% of max_steps
    'checkpoint_freq': 50,          # More frequent checkpoints
    'eval_freq': 50,                # Frequent evaluation
    'weight_decay': 0.01,           # Regularization
    'gradient_clip': 1.0,           # Prevent instability
}
```

#### Loss Masking (Optional but Recommended)

Only compute loss on the response portion:

```python
# Mask the instruction part, only train on response
instruction_tokens = tokenize("### Instruction:\n{instruction}\n\n### Response:\n")
labels = input_ids.clone()
labels[:len(instruction_tokens)] = -100  # Ignore loss for instruction
```

This helps the model focus on generating good responses rather than memorizing instructions.

### Phase 3: Create Fine-tuning Script

We'll adapt your existing `train_improved.py`:

**New file**: `train_instruct.py`

Key modifications:
1. Load instruction dataset instead of FineWeb-Edu
2. Format data with chat template
3. Adjust hyperparameters (lower LR, more epochs)
4. Add loss masking (optional)
5. Keep all the good stuff (checkpointing, TensorBoard, DDP)

---

## ğŸ¯ Step-by-Step Implementation Plan

### Step 1: Prepare Dataset (30 minutes)
```bash
# Create new script to download and prepare instruction dataset
python src/prepare_instruct_dataset.py --dataset dolly-15k
```

**What it does**:
- Downloads Dolly-15k from Hugging Face
- Formats with chat template
- Tokenizes and saves shards
- Creates train/val splits (90/10)

### Step 2: Create Training Script (1 hour)
```bash
# Copy and modify existing training script
cp src/train_improved.py src/train_instruct.py
```

**Modifications needed**:
- Change dataloader to read instruction format
- Lower learning rate (2e-5 instead of 6e-4)
- Adjust max_steps for multiple epochs
- Add instruction/response separation (optional)

### Step 3: Start Fine-tuning (2-4 hours)
```bash
# Single GPU
python src/train_instruct.py \
    --checkpoint_to_load checkpoints/best_model.pt \
    --learning_rate 2e-5 \
    --max_steps 500 \
    --use_tensorboard

# Or 4 GPUs (faster)
torchrun --standalone --nproc_per_node=4 src/train_instruct.py \
    --checkpoint_to_load checkpoints/best_model.pt \
    --learning_rate 2e-5 \
    --max_steps 500 \
    --use_tensorboard
```

### Step 4: Create Chat Interface (30 minutes)
```bash
# Create interactive chat script
python src/chat.py --checkpoint checkpoints_instruct/best_model.pt
```

### Step 5: Evaluate & Iterate (ongoing)
- Test with various prompts
- Check if responses are helpful
- Adjust hyperparameters if needed
- Try different datasets

---

## ğŸ“ Expected Results

### What to Expect from Your Small GPT-2

âœ… **Will do well**:
- Simple Q&A
- Short explanations
- Basic instructions
- Creative short-form writing
- Factual recall from training data

âš ï¸ **Will struggle with**:
- Very complex reasoning
- Long-form generation
- Up-to-date information (limited by pre-training data)
- Multi-step instructions
- Advanced math/coding

### Realistic Example Outputs

**Good**:
```
User: What is Python?
Model: Python is a high-level programming language known for its simplicity 
       and readability. It's widely used for web development, data analysis, 
       and machine learning.
```

**Okay**:
```
User: How do I sort a list in Python?
Model: You can use the sorted() function or the .sort() method. For example, 
       sorted([3,1,2]) returns [1,2,3].
```

**May struggle**:
```
User: Explain quantum computing and how it differs from classical computing 
      in terms of computational complexity theory.
Model: [May produce simplified or incomplete answers]
```

---

## ğŸ› ï¸ Technical Details

### Memory Requirements

| Configuration | Memory Usage | Training Time (Dolly-15k) |
|--------------|--------------|--------------------------|
| Single GPU | ~40-50 GB | 3-4 hours |
| 4 GPUs | ~40-50 GB per GPU | 1-2 hours |

You have 120GB available, so plenty of headroom!

### Dataset Statistics

**Dolly-15k breakdown**:
- Open Q&A: 3,271 examples
- Closed Q&A: 4,352 examples
- Creative Writing: 2,316 examples
- Information Extraction: 1,845 examples
- Summarization: 3,216 examples

**Total tokens** (estimated): ~5-7M tokens
- **vs. Pre-training**: 10B tokens (2000x less!)
- **Why it works**: Model already knows language, just learning format

### Cost Comparison

| Phase | Data Size | Time | Cost (approx) |
|-------|-----------|------|---------------|
| Pre-training | 10B tokens | 34 days | $$$$$ |
| Instruction Tuning | 5M tokens | 4 hours | $ |

**Ratio**: Instruction tuning is ~200x faster and cheaper!

---

## ğŸ¨ Inference with Instruction Format

### Update `inference.py` for Chat

```python
def chat_inference(model, tokenizer, instruction, max_tokens=150):
    # Format with template
    prompt = f"### Instruction:\n{instruction}\n\n### Response:\n"
    
    # Tokenize
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    
    # Generate
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_length=len(input_ids[0]) + max_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode and extract response
    full_text = tokenizer.decode(output[0], skip_special_tokens=True)
    response = full_text.split("### Response:\n")[-1]
    
    return response
```

### Interactive Chat Script

```python
# chat.py
while True:
    user_input = input("\nYou: ")
    if user_input.lower() in ['exit', 'quit']:
        break
    
    response = chat_inference(model, tokenizer, user_input)
    print(f"\nAssistant: {response}")
```

---

## ğŸ¯ Success Metrics

### How to Evaluate

1. **Qualitative Testing** (Most important for chat)
   - Try 20-30 diverse prompts
   - Rate responses: helpful, harmless, honest
   - Compare to base model

2. **Quantitative Metrics** (Optional)
   - Perplexity on validation set
   - ROUGE/BLEU scores (if you have reference answers)
   - Training loss curve

3. **User Testing**
   - Have others try the chat
   - Collect feedback
   - Iterate

### Red Flags to Watch For

âŒ **Overfitting**: Model just memorizes training responses
- **Solution**: Early stopping, lower learning rate

âŒ **Catastrophic Forgetting**: Model forgets pre-training knowledge
- **Solution**: Lower learning rate, fewer epochs

âŒ **Format Obsession**: Model always outputs template format
- **Solution**: Add diverse formats to dataset

---

## ğŸš€ Next Steps: Prioritized

### Immediate (Today)
1. âœ… Review this plan
2. ğŸ“¥ Download Dolly-15k dataset
3. ğŸ”§ Create `prepare_instruct_dataset.py`
4. ğŸ§ª Test data formatting on a few examples

### Short-term (Tomorrow)
5. ğŸ› ï¸ Adapt `train_improved.py` â†’ `train_instruct.py`
6. ğŸš€ Start fine-tuning run
7. ğŸ“Š Monitor TensorBoard
8. ğŸ’¾ Save checkpoints

### Medium-term (This Week)
9. ğŸ§ª Test inference with instruction format
10. ğŸ’¬ Create interactive chat script
11. ğŸ“ Evaluate responses qualitatively
12. ğŸ”„ Iterate if needed

### Optional Enhancements
- Try Alpaca dataset for more diversity
- Add system prompts for behavior control
- Implement loss masking for better training
- Create web interface with Gradio
- Fine-tune on your own custom dataset

---

## ğŸ“š Resources & References

### Datasets
- [Dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
- [Alpaca GPT-4](https://huggingface.co/datasets/vicgalle/alpaca-gpt4)
- [Anthropic HH](https://huggingface.co/datasets/Anthropic/hh-rlhf)

### Papers
- "Self-Instruct" (Wang et al., 2023) - Generating instruction data
- "Alpaca" (Stanford, 2023) - Instruction tuning for LLaMA
- "InstructGPT" (OpenAI, 2022) - RLHF and instruction following

### Examples
- [Fine-tuned GPT-2 Instruct Models on HF](https://huggingface.co/models?search=gpt2%20instruct)
- [RaushanTurganbay/GPT2_instruct_tuned](https://huggingface.co/RaushanTurganbay/GPT2_instruct_tuned)

---

## ğŸ’¡ Pro Tips

1. **Start Small**: Use Dolly-15k first, it's enough for good results
2. **Monitor Loss**: It should drop quickly (within 100 steps)
3. **Test Early**: Generate samples every 50 steps to see progress
4. **Compare**: Keep base model for comparison
5. **Temperature**: Use 0.7-0.9 for chat (more creative than pre-training)
6. **Stop Early**: 3 epochs is usually enough, more risks overfitting

---

## ğŸ¬ What We'll Build Together

I'll help you create:

1. âœ… `prepare_instruct_dataset.py` - Download & format data
2. âœ… `train_instruct.py` - Fine-tuning script
3. âœ… `chat.py` - Interactive chat interface
4. âœ… `inference_instruct.py` - Batch inference with instruction format
5. âœ… Documentation & usage guides

**Ready to start?** Let me know and I'll begin implementing! ğŸš€

---

## â“ Questions to Consider

Before we start, please confirm:

1. **Dataset**: Start with Dolly-15k (15k samples, ~4 hours)? Or Alpaca (52k samples, ~12 hours)?
2. **Training**: Single GPU or 4 GPUs?
3. **Goal**: General assistant or specialized (e.g., coding, education)?
4. **Evaluation**: What kind of questions/tasks do you want to test?

Let me know your preferences and we'll begin implementation! ğŸ¯

