# FP8 Training Guide

## ðŸš€ Overview

`train_fp8.py` enables **FP8 (8-bit floating point) training** on NVIDIA Blackwell GPUs using TransformerEngine. This can provide **~2x speedup** compared to bfloat16 training.

**Key Features:**
- âœ… All checkpointing features (Phase 1)
- âœ… All TensorBoard monitoring (Phase 2)
- âœ… **NEW:** FP8 precision training
- âœ… Automatic fallback to bfloat16 if FP8 unavailable
- âœ… Separate checkpoint directory (`./checkpoints_fp8`)

---

## âš¡ Expected Speedup

| Precision | Speed | Quality | Use Case |
|-----------|-------|---------|----------|
| **bfloat16** | 1.0x (baseline) | Excellent | Standard training |
| **FP8 (E4M3)** | ~1.5-2.0x | Very Good | Production (recommended) |
| **FP8 (Hybrid)** | ~1.5-2.0x | Excellent | Best balance |

**For your 34-day training:** 34 days â†’ **17-23 days** with FP8! ðŸŽ‰

---

## ðŸ“‹ Prerequisites

### 1. Hardware Requirements

**FP8 requires:**
- NVIDIA Blackwell GPUs (GB100, GB200)
- OR NVIDIA Hopper GPUs (H100, H200)
- Compute Capability 9.0+

**Check your GPU:**
```bash
nvidia-smi --query-gpu=name,compute_cap --format=csv
```

**Expected output:**
```
name, compute_cap
NVIDIA GB100, 9.0
```

If compute capability < 9.0, FP8 will **not be available**.

### 2. Software Requirements

**Install TransformerEngine:**

```bash
# Install prerequisites
pip install pybind11

# Install TransformerEngine from source
pip install git+https://github.com/NVIDIA/TransformerEngine.git
```

**Or from NVIDIA NGC (if available):**
```bash
pip install transformer-engine[pytorch]
```

**Verify installation:**
```bash
python -c "import transformer_engine.pytorch as te; print('TransformerEngine:', te.__version__)"
```

---

## ðŸš€ Quick Start

### Basic FP8 Training

```bash
# Single GPU
python src/train_fp8.py --use_fp8 --use_tensorboard

# Multi-GPU (4 GPUs)
torchrun --standalone --nproc_per_node=4 src/train_fp8.py \
  --use_fp8 \
  --use_tensorboard \
  --run_name "gpt2_fp8_test"
```

### With Custom Configuration

```bash
torchrun --standalone --nproc_per_node=4 src/train_fp8.py \
  --use_fp8 \
  --fp8_format hybrid \
  --use_tensorboard \
  --run_name "gpt2_fp8_production" \
  --checkpoint_freq 250 \
  --eval_freq 250
```

### Fallback Mode (No FP8)

If TransformerEngine is not installed, the script automatically falls back to bfloat16:

```bash
python src/train_fp8.py --use_tensorboard
# Will use bfloat16 automatically
```

---

## ðŸ”§ FP8 Configuration Options

### FP8 Format

```bash
--fp8_format {hybrid,e4m3,e5m2}
```

**Options:**

1. **`hybrid`** (Default, Recommended)
   - E4M3 for forward pass
   - E5M2 for backward pass
   - **Best balance** of speed and quality
   - Use this unless you have specific reasons

2. **`e4m3`**
   - 4-bit exponent, 3-bit mantissa
   - Better for forward pass
   - Slightly less precise for gradients
   - Use if forward pass is bottleneck

3. **`e5m2`**
   - 5-bit exponent, 2-bit mantissa
   - Better dynamic range
   - Better for gradients
   - Use if training is unstable

### FP8 Scaling Parameters

```bash
--fp8_margin 0        # Scaling margin (default: 0)
--fp8_interval 1      # Amax history interval (default: 1)
```

**When to adjust:**
- If training is unstable â†’ increase `fp8_margin` to 1 or 2
- If seeing NaN losses â†’ use `--fp8_format hybrid` or `e5m2`

---

## ðŸ“ File Organization

### Separate Checkpoint Directories

By default, FP8 training uses a **separate checkpoint directory**:

```
.
â”œâ”€â”€ checkpoints/           # bfloat16 checkpoints (train_improved.py)
â”‚   â”œâ”€â”€ latest.pt
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ checkpoints_fp8/       # FP8 checkpoints (train_fp8.py)
â”‚   â”œâ”€â”€ latest.pt
â”‚   â”œâ”€â”€ best_model.pt
â”‚   â””â”€â”€ ...
â””â”€â”€ runs/                  # TensorBoard logs (both)
    â”œâ”€â”€ gpt2_bf16_...
    â””â”€â”€ gpt2_fp8_...
```

**Why separate?**
- Keeps experimental FP8 separate from stable bfloat16
- Prevents accidental mixing
- Easy to compare both versions

**Custom checkpoint directory:**
```bash
python src/train_fp8.py --use_fp8 --checkpoint_dir ./my_fp8_checkpoints
```

---

## ðŸ§ª Testing FP8 Before Full Training

**Before starting a 34-day run, test FP8 quality!**

### Short Test Run (100 Steps)

```bash
python src/train_fp8.py \
  --use_fp8 \
  --use_tensorboard \
  --run_name "fp8_test_100steps" \
  --checkpoint_freq 25 \
  --eval_freq 25
```

Let it run for 100-200 steps, then check:

1. **Loss values:** Should be similar to bfloat16
2. **Throughput:** Should be ~1.5-2x faster
3. **No NaN losses:** Training should be stable
4. **Generated text:** Should make sense

### Compare FP8 vs bfloat16

**Run both for 1000 steps:**

**Terminal 1 (bfloat16):**
```bash
python src/train_improved.py \
  --use_tensorboard \
  --run_name "compare_bf16"
```

**Terminal 2 (FP8):**
```bash
python src/train_fp8.py \
  --use_fp8 \
  --use_tensorboard \
  --run_name "compare_fp8"
```

**Compare in TensorBoard:**
```bash
tensorboard --logdir=runs
```

**Look for:**
- Loss curves should be nearly identical
- FP8 should be faster (higher tokens/sec)
- Quality metrics should be similar

---

## ðŸ“Š Monitoring FP8 Training

### TensorBoard

FP8 training logs the same metrics as bfloat16:

```bash
./start_tensorboard.sh
```

**Additional info:**
- Training output shows: `| FP8` (instead of `| BF16`)
- Checkpoint metadata includes precision type
- TensorBoard run name includes `fp8` tag

### Log Output Example

```
step  250 | epoch 0 | loss: 3.456789 | lr: 6.00e-04 | norm: 0.8234 | dt: 156.78ms | tok/sec: 3345.67 | FP8
```

Note the **faster step time** and **higher throughput** compared to bfloat16!

---

## ðŸ”„ Resume FP8 Training

Works exactly like bfloat16:

```bash
torchrun --standalone --nproc_per_node=4 src/train_fp8.py \
  --resume latest \
  --use_fp8 \
  --use_tensorboard
```

**The script will:**
- âœ… Load from `checkpoints_fp8/latest.pt`
- âœ… Restore FP8 precision setting
- âœ… Continue training seamlessly

---

## âš ï¸ Important Notes

### 1. Checkpoint Compatibility

**FP8 â†’ bfloat16:** âœ… Can load FP8 checkpoint in bfloat16 mode
**bfloat16 â†’ FP8:** âœ… Can load bfloat16 checkpoint in FP8 mode

**Example:**
```bash
# Train 1000 steps in bfloat16
python src/train_improved.py

# Continue in FP8
python src/train_fp8.py --use_fp8 --resume checkpoints/latest.pt --checkpoint_dir checkpoints_fp8
```

### 2. Quality Validation

**Always validate FP8 quality before full training:**
1. Run short test (100-1000 steps)
2. Compare loss curves with bfloat16
3. Check generated text quality
4. Verify no instability

### 3. Hardware Support

**If FP8 is not supported:**
- Script will print warning
- Automatically falls back to bfloat16
- Training continues normally

---

## ðŸ› Troubleshooting

### Problem: "TransformerEngine not available"

**Solution:**
```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git
```

### Problem: NaN Losses with FP8

**Solutions:**
1. Try hybrid format: `--fp8_format hybrid`
2. Increase margin: `--fp8_margin 1`
3. Reduce learning rate: `--max_lr 5e-4`
4. Fall back to bfloat16 (remove `--use_fp8`)

### Problem: FP8 not actually faster

**Check:**
1. Is TransformerEngine actually installed?
2. Is GPU compute capability >= 9.0?
3. Is batch size large enough? (FP8 shines with larger batches)

**Verify FP8 is active:**
```bash
# Should see "ðŸš€ FP8 Training Enabled!" in output
# Training output should show "| FP8" suffix
```

### Problem: Loss curves different from bfloat16

**Small differences are normal (<5%).**

If differences are large (>10%):
1. Try `--fp8_format hybrid`
2. Validate quality metrics
3. Consider staying with bfloat16

---

## ðŸ“ˆ Performance Expectations

### Throughput Improvement

**Typical results on Blackwell GB100:**

| Setup | bfloat16 | FP8 | Speedup |
|-------|----------|-----|---------|
| **Single GPU** | 2000 tok/sec | 3500 tok/sec | 1.75x |
| **4 GPUs** | 8000 tok/sec | 14000 tok/sec | 1.75x |
| **8 GPUs** | 15000 tok/sec | 27000 tok/sec | 1.80x |

**Your 34-day training:**
- bfloat16: 34 days
- FP8: **19-23 days** (estimated)
- **Savings: 11-15 days!** â°

### Memory Usage

**FP8 uses ~20-30% less GPU memory:**
- Can fit larger batches
- Or train larger models
- Less OOM (out of memory) errors

### Quality

**Expected quality:**
- Training loss: Within 1-2% of bfloat16
- Validation loss: Within 2-3% of bfloat16
- HellaSwag: Within 1% of bfloat16
- Generated text: Comparable quality

---

## ðŸ’¡ Best Practices

### 1. Start with Default Settings

```bash
python src/train_fp8.py --use_fp8 --use_tensorboard
```

Use defaults (`hybrid` format) unless you have issues.

### 2. Test Before Production

Always run a short test (100-1000 steps) before committing to a 34-day run.

### 3. Monitor Closely

Check TensorBoard daily for first few days of FP8 training to catch any issues early.

### 4. Keep Checkpoints Safe

FP8 is still experimental. Keep regular backups of important checkpoints.

### 5. Compare with Baseline

If possible, run bfloat16 in parallel for first epoch to compare quality.

---

## ðŸŽ¯ Recommended Workflow

### Step 1: Verify Hardware Support

```bash
nvidia-smi --query-gpu=name,compute_cap --format=csv
```

Ensure compute capability >= 9.0.

### Step 2: Install TransformerEngine

```bash
pip install git+https://github.com/NVIDIA/TransformerEngine.git
```

### Step 3: Quick Test (10 minutes)

```bash
python src/train_fp8.py --use_fp8 --use_tensorboard --eval_freq 10 --checkpoint_freq 10
```

Let run for 50-100 steps. Check:
- âœ… No errors
- âœ… Training is stable
- âœ… Throughput is higher

### Step 4: Validation Run (1-2 hours)

```bash
torchrun --standalone --nproc_per_node=4 src/train_fp8.py \
  --use_fp8 \
  --use_tensorboard \
  --run_name "fp8_validation_1000steps"
```

Let run for 1000 steps. Check TensorBoard:
- âœ… Loss curve looks good
- âœ… No NaN values
- âœ… Generated text makes sense
- âœ… ~1.5-2x faster than bfloat16

### Step 5: Production Run (17-23 days)

```bash
torchrun --standalone --nproc_per_node=4 src/train_fp8.py \
  --use_fp8 \
  --fp8_format hybrid \
  --use_tensorboard \
  --run_name "gpt2_fp8_production_17days" \
  --checkpoint_freq 250 \
  --eval_freq 250
```

---

## ðŸ“š Command Reference

### Start FP8 Training

```bash
# Single GPU
python src/train_fp8.py --use_fp8 --use_tensorboard

# Multi-GPU
torchrun --standalone --nproc_per_node=4 src/train_fp8.py --use_fp8 --use_tensorboard
```

### Resume FP8 Training

```bash
python src/train_fp8.py --resume latest --use_fp8 --use_tensorboard
```

### List FP8 Checkpoints

```bash
python src/train_fp8.py --list_checkpoints
```

### Switch from bfloat16 to FP8

```bash
# Start in bfloat16
python src/train_improved.py --use_tensorboard

# After 1000 steps, switch to FP8
python src/train_fp8.py --use_fp8 --resume checkpoints/latest.pt --checkpoint_dir checkpoints_fp8 --use_tensorboard
```

---

## âœ… FP8 Checklist

Before starting production FP8 training:

**Hardware:**
- [ ] GPU is Blackwell or Hopper (compute capability >= 9.0)
- [ ] GPU memory is sufficient (same as bfloat16)

**Software:**
- [ ] TransformerEngine installed and working
- [ ] Can import: `import transformer_engine.pytorch`

**Validation:**
- [ ] Ran 100-step test successfully
- [ ] Compared FP8 vs bfloat16 quality (1000 steps)
- [ ] Loss curves are similar
- [ ] No NaN losses or instability
- [ ] Generated text quality is good
- [ ] Throughput is 1.5-2x higher

**Infrastructure:**
- [ ] Separate checkpoint directory configured
- [ ] TensorBoard monitoring active
- [ ] Resume functionality tested

**Backup Plan:**
- [ ] Know how to switch back to bfloat16
- [ ] Have bfloat16 baseline checkpoints

---

## ðŸŽ‰ Summary

**FP8 Training with `train_fp8.py`:**
- âœ… All checkpointing features preserved
- âœ… All TensorBoard monitoring preserved
- âœ… ~1.5-2x faster training (34 days â†’ 17-23 days)
- âœ… Separate checkpoint directory (safe)
- âœ… Automatic fallback to bfloat16
- âœ… Easy to test and validate

**Start testing now:**

```bash
python src/train_fp8.py --use_fp8 --use_tensorboard --checkpoint_freq 25 --eval_freq 25
```

**Good luck with your accelerated training!** ðŸš€âš¡

